{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c04cf0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "from utils.utils import save_experiment, train_and_evaluate_logistic_regression, train_and_evaluate_linear_svm, train_and_evaluate_non_linear_svm, train_and_evaluate_decision_tree, train_and_evaluate_random_forest, train_and_evaluate_xgboost\n",
    "from configs.config import DATASET_PATH, FEATURES_DIR, ITW_DATASET_PATH, MODELS_PATH\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d672f63",
   "metadata": {},
   "source": [
    "### Parquet paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b0d3c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = os.path.join(FEATURES_DIR, \"training_features_mean_20_128_256_128.parquet\")\n",
    "test_data_path = os.path.join(FEATURES_DIR, \"testing_features_mean_20_128_256_128.parquet\")\n",
    "itw_data_path = os.path.join(ITW_DATASET_PATH, 'normalized_features', \"itw_features_mean_20_128_256_128_trimmed_loudness_normalized.parquet\")\n",
    "\n",
    "#no mel features\n",
    "train_data_path_no_mel = os.path.join(FEATURES_DIR, \"training_features_mean_20_128_256_128_no_mel.parquet\")\n",
    "test_data_path_no_mel = os.path.join(FEATURES_DIR, \"testing_features_mean_20_128_256_128_no_mel.parquet\")\n",
    "itw_data_path_no_mel = os.path.join(ITW_DATASET_PATH, 'normalized_features', \"itw_features_mean_20_128_256_128_no_mel_trimmed_loudness_normalized.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18599dba",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a98da98",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\konst\\\\Documents\\\\GitHub\\\\audio-deepfake-detection\\\\in-the-wild-audio-deepfake\\\\normalized_features\\\\itw_features_mean_20_128_256_128_trimmed_loudness_normalized.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#params: {'clf__C': np.float64(2.1544346900318843), 'clf__max_iter': 1000, 'clf__penalty': 'l2', 'clf__solver': 'saga'}\u001b[39;00m\n\u001b[32m      4\u001b[39m lr_params = {\n\u001b[32m      5\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mC\u001b[39m\u001b[33m\"\u001b[39m: np.float64(\u001b[32m2.1544346900318843\u001b[39m),  \u001b[38;5;66;03m# Regularization strength\u001b[39;00m\n\u001b[32m      6\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mclass_weight\u001b[39m\u001b[33m\"\u001b[39m:  {\u001b[32m0\u001b[39m: \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m: \u001b[32m5\u001b[39m},  \u001b[38;5;66;03m# Handle imbalanced classes\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpenalty\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33ml2\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# Standard L2 regularization\u001b[39;00m\n\u001b[32m     11\u001b[39m         }\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m pipeline, metrics, lr_params, feature_names, metadata_extra = \u001b[43mtrain_and_evaluate_logistic_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitw_data_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m save_experiment(\n\u001b[32m     16\u001b[39m     model=pipeline,\n\u001b[32m     17\u001b[39m     metrics=metrics,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     metadata_extra=metadata_extra,\n\u001b[32m     22\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\konst\\Documents\\GitHub\\audio-deepfake-detection\\utils\\utils.py:235\u001b[39m, in \u001b[36mtrain_and_evaluate_logistic_regression\u001b[39m\u001b[34m(train_path, test_path, lr_params)\u001b[39m\n\u001b[32m    233\u001b[39m train_df = pd.read_parquet(train_path)\n\u001b[32m    234\u001b[39m train_df.dropna(inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m test_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m test_df.dropna(inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    238\u001b[39m \u001b[38;5;66;03m# Split features and labels\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\konst\\anaconda3\\envs\\audio_deepfake_py313\\Lib\\site-packages\\pandas\\io\\parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\konst\\anaconda3\\envs\\audio_deepfake_py313\\Lib\\site-packages\\pandas\\io\\parquet.py:258\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    257\u001b[39m     to_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33msplit_blocks\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    265\u001b[39m     pa_table = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    266\u001b[39m         path_or_handle,\n\u001b[32m    267\u001b[39m         columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    270\u001b[39m         **kwargs,\n\u001b[32m    271\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\konst\\anaconda3\\envs\\audio_deepfake_py313\\Lib\\site-packages\\pandas\\io\\parquet.py:141\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    131\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    134\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\konst\\anaconda3\\envs\\audio_deepfake_py313\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\konst\\\\Documents\\\\GitHub\\\\audio-deepfake-detection\\\\in-the-wild-audio-deepfake\\\\normalized_features\\\\itw_features_mean_20_128_256_128_trimmed_loudness_normalized.parquet'"
     ]
    }
   ],
   "source": [
    "#params: {'clf__C': np.float64(2.1544346900318843), 'clf__max_iter': 1000, 'clf__penalty': 'l2', 'clf__solver': 'saga'}\n",
    "\n",
    "\n",
    "lr_params = {\n",
    "            \"C\": np.float64(2.1544346900318843),  # Regularization strength\n",
    "            \"class_weight\":  {0: 1, 1: 5},  # Handle imbalanced classes\n",
    "            \"max_iter\": 1000,  # Usually enough to converge\n",
    "            \"random_state\": 42,\n",
    "            \"solver\": \"saga\",  # Good for small-medium datasets, handles binary classification well\n",
    "            \"penalty\": \"l2\",  # Standard L2 regularization\n",
    "        }\n",
    "\n",
    "pipeline, metrics, lr_params, feature_names, metadata_extra = train_and_evaluate_logistic_regression(\n",
    "    train_data_path, \n",
    "    itw_data_path, \n",
    "    lr_params)\n",
    "\n",
    "print(metadata_extra)\n",
    "print(metrics)\n",
    "\n",
    "save_experiment(\n",
    "    model=pipeline,\n",
    "    metrics=metrics,\n",
    "    experiment_dir=os.path.join(sys.path[0], \"notebooks\", \"experiments\",  \"logistic_reg\",),\n",
    "    model_params=lr_params,\n",
    "    feature_names=feature_names,\n",
    "    metadata_extra=metadata_extra,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ae807",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec23205",
   "metadata": {},
   "outputs": [],
   "source": [
    "#params: {'svm__C': 0.01}\n",
    "\n",
    "svc_params = {\n",
    "            \"C\": 0.01,\n",
    "            \"class_weight\":  {0: 1, 1: 5},\n",
    "            \"max_iter\": 20000,\n",
    "            \"random_state\": 42,\n",
    "        }\n",
    "\n",
    "pipeline, metrics, svc_params, feature_names, metadata_extra = train_and_evaluate_linear_svm(\n",
    "    train_data_path, \n",
    "    itw_data_path, \n",
    "    svc_params)\n",
    "\n",
    "print(metadata_extra)\n",
    "print(metrics)\n",
    "\n",
    "save_experiment(\n",
    "    model=pipeline,\n",
    "    metrics=metrics,\n",
    "    experiment_dir=os.path.join(sys.path[0], \"notebooks\", \"experiments\",  \"linear_svm\",),\n",
    "    model_params=svc_params,\n",
    "    feature_names=feature_names,\n",
    "    metadata_extra=metadata_extra,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84642d75",
   "metadata": {},
   "source": [
    "### RBF SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13d1926",
   "metadata": {},
   "outputs": [],
   "source": [
    "#params: {'svm__C': 0.1, 'svm__gamma': 0.01, 'svm__kernel': 'rbf'}\n",
    "svm_params = {\n",
    "            \"kernel\": \"rbf\",\n",
    "            \"C\": 0.1,\n",
    "            \"gamma\":0.01,\n",
    "            \"class_weight\": {0: 1, 1: 5},\n",
    "            \"random_state\": 42,\n",
    "        }\n",
    "\n",
    "pipeline, metrics, svm_params, feature_names, metadata_extra = train_and_evaluate_non_linear_svm(\n",
    "    train_data_path, \n",
    "    itw_data_path, \n",
    "    svm_params)\n",
    "\n",
    "print(metadata_extra)\n",
    "print(metrics)\n",
    "\n",
    "save_experiment(\n",
    "    model=pipeline,\n",
    "    metrics=metrics,\n",
    "    experiment_dir=os.path.join(sys.path[0], \"notebooks\", \"experiments\",  \"rbf_svm\",),\n",
    "    model_params=svm_params,\n",
    "    feature_names=feature_names,\n",
    "    metadata_extra=metadata_extra,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a62051",
   "metadata": {},
   "source": [
    "### Poly SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6a605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#params: {'svm__C': 1, 'svm__coef0': 0.0, 'svm__degree': 2, 'svm__gamma': 0.01, 'svm__kernel': 'poly'}\n",
    "\n",
    "svm_params = {\n",
    "            \"kernel\": \"poly\",\n",
    "            \"C\": 1,\n",
    "            \"coef0\": 0.0,\n",
    "            \"degree\":2,\n",
    "            \"gamma\":0.01,\n",
    "            \"class_weight\": {0: 1, 1: 5},\n",
    "            \"random_state\": 42,\n",
    "        }\n",
    "\n",
    "pipeline, metrics, svm_params, feature_names, metadata_extra = train_and_evaluate_non_linear_svm(\n",
    "    train_data_path, \n",
    "    itw_data_path, \n",
    "    svm_params)\n",
    "\n",
    "print(metadata_extra)\n",
    "print(metrics)\n",
    "\n",
    "save_experiment(\n",
    "    model=pipeline,\n",
    "    metrics=metrics,\n",
    "    experiment_dir=os.path.join(sys.path[0], \"notebooks\", \"experiments\",  \"poly_svm\",),\n",
    "    model_params=svm_params,\n",
    "    feature_names=feature_names,\n",
    "    metadata_extra=metadata_extra,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcb025d",
   "metadata": {},
   "source": [
    "### Sigmoid Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb036435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#params: {'svm__C': 0.1, 'svm__coef0': -1.0, 'svm__gamma': 'scale', 'svm__kernel': 'sigmoid'}\n",
    "\n",
    "svm_params = {\n",
    "            \"kernel\": \"sigmoid\",\n",
    "            \"C\": 0.1,\n",
    "            \"coef0\": -1.0,\n",
    "            \"gamma\":'scale',\n",
    "            \"class_weight\": {0: 1, 1: 5},\n",
    "            \"random_state\": 42,\n",
    "        }\n",
    "\n",
    "pipeline, metrics, svm_params, feature_names, metadata_extra = train_and_evaluate_non_linear_svm(\n",
    "    train_data_path,\n",
    "    itw_data_path, \n",
    "    svm_params)\n",
    "\n",
    "print(metadata_extra)\n",
    "print(metrics)\n",
    "\n",
    "save_experiment(\n",
    "    model=pipeline,\n",
    "    metrics=metrics,\n",
    "    experiment_dir=os.path.join(sys.path[0], \"notebooks\", \"experiments\",  \"poly_svm\",),\n",
    "    model_params=svm_params,\n",
    "    feature_names=feature_names,\n",
    "    metadata_extra=metadata_extra,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d727bcbd",
   "metadata": {},
   "source": [
    "### Decission Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30dd261",
   "metadata": {},
   "outputs": [],
   "source": [
    "params= {'class_weight': {0: 1, 1: 5},\n",
    "         'criterion': 'gini',\n",
    "         'max_depth': 10,\n",
    "         'min_samples_leaf': 2,\n",
    "         'min_samples_split': 2\n",
    "         }\n",
    "\n",
    "\n",
    "clf, metrics, dt_params, feature_names, metadata_extra = train_and_evaluate_decision_tree(\n",
    "    train_path=train_data_path,\n",
    "    val_path=None,\n",
    "    test_path=itw_data_path,\n",
    "    dt_params=params\n",
    ")\n",
    "\n",
    "save_experiment(\n",
    "    model=pipeline,\n",
    "    metrics=metrics,\n",
    "    experiment_dir=os.path.join(sys.path[0], \"notebooks\", \"experiments\",  \"Dtree\",),\n",
    "    model_params=params,\n",
    "    feature_names=feature_names,\n",
    "    metadata_extra=metadata_extra,\n",
    ")\n",
    "print(metadata_extra)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ba882",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c76dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'class_weight': {0: 1, 1: 5},\n",
    "     'max_depth': None,\n",
    "     'max_features': 'sqrt',\n",
    "     'min_samples_leaf': 4,\n",
    "     'n_estimators': 300\n",
    "     }\n",
    "\n",
    "\n",
    "pipeline, metrics, rf_params, feature_names, metadata_extra, oob_score = train_and_evaluate_random_forest(\n",
    "    train_path=train_data_path,\n",
    "    val_path=None,\n",
    "    test_path=itw_data_path,\n",
    "    rf_params=params\n",
    ")\n",
    "\n",
    "save_experiment(\n",
    "    model=pipeline,\n",
    "    metrics=metrics,\n",
    "    experiment_dir=os.path.join(sys.path[0], \"notebooks\", \"experiments\",  \"RF\",),\n",
    "    model_params=rf_params,\n",
    "    feature_names=feature_names,\n",
    "    metadata_extra=metadata_extra,\n",
    ")\n",
    "\n",
    "print(\"Metadata:\", metadata_extra)\n",
    "print(\"Metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcbc977",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3c0406",
   "metadata": {},
   "source": [
    "#### XGBoost Mel features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad24f80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": 4,\n",
    "    \"learning_rate\": 0.25,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"scale_pos_weight\": 2,\n",
    "    \"eval_metric\": \"aucpr\",\n",
    "    \"random_state\": 42,\n",
    "    \"objective\":\"binary:logistic\"\n",
    "}\n",
    "\n",
    "pipeline, metrics, xgb_params, feature_names, metadata_extra = train_and_evaluate_xgboost(\n",
    "    train_path=train_data_path_no_mel,\n",
    "    val_path=test_data_path_no_mel,\n",
    "    test_path=itw_data_path_no_mel,\n",
    "    xgb_params=params\n",
    ")\n",
    "\n",
    "\n",
    "save_experiment(\n",
    "    pipeline,\n",
    "    metrics,\n",
    "    experiment_dir=os.path.join(\"experiments\", \"XGB\"),\n",
    "    model_params=xgb_params,\n",
    "    feature_names=feature_names,\n",
    "    metadata_extra=metadata_extra,\n",
    ")\n",
    "\n",
    "print(\"Metadata:\", metadata_extra)\n",
    "print(\"Metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f48532",
   "metadata": {},
   "source": [
    "#### XGBoost without mel features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b567f037",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_depth\": 4,\n",
    "    \"learning_rate\": 0.25,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"scale_pos_weight\": 2,\n",
    "    \"eval_metric\": \"aucpr\",\n",
    "    \"random_state\": 42,\n",
    "    #\"objective\":\"binary:logistic\"\n",
    "}\n",
    "\n",
    "pipeline, metrics, xgb_params, feature_names, metadata_extra = train_and_evaluate_xgboost(\n",
    "    train_path=train_data_path,\n",
    "    val_path=test_data_path,\n",
    "    test_path=itw_data_path,\n",
    "    xgb_params=params\n",
    ")\n",
    "\n",
    "\n",
    "save_experiment(\n",
    "    pipeline,\n",
    "    metrics,\n",
    "    experiment_dir=os.path.join(\"experiments\", \"XGB_no_mel\"),\n",
    "    model_params=xgb_params,\n",
    "    feature_names=feature_names,\n",
    "    metadata_extra=metadata_extra,\n",
    ")\n",
    "\n",
    "print(\"Metadata:\", metadata_extra)\n",
    "print(\"Metrics:\", metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_deepfake_py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
