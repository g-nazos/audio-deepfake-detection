{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import gradio as gr\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "import torch\n",
    "import librosa\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1a33083c7144c69bf6cdb95cfbbdf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962ad297122d4312a29bb296a9f7a017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d2e9d3157b4b73a4a0f23ed4a94644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Load model and processor\n",
    "model_name = \"prithivMLmods/Common-Voice-Geneder-Detection\"\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name)\n",
    "processor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model.to(device)  # Move model to GPU\n",
    "model.eval()      # Set to evaluation mode\n",
    "\n",
    "\n",
    "\n",
    "# Label mapping\n",
    "id2label = {\n",
    "    \"0\": \"female\",\n",
    "    \"1\": \"male\"\n",
    "}\n",
    "\n",
    "def classify_audio(audio_path):\n",
    "    # Load and resample audio to 16kHz\n",
    "    speech, sample_rate = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "    # Process audio\n",
    "    inputs = processor(\n",
    "        speech,\n",
    "        sampling_rate=sample_rate,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.nn.functional.softmax(logits, dim=1).squeeze().tolist()\n",
    "\n",
    "    prediction = {\n",
    "        id2label[str(i)]: round(probs[i], 3) for i in range(len(probs))\n",
    "    }\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/d/for-dataset/for-original/for-original/training/real'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2792498239.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresults_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# Store predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path_train_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only process audio files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path_train_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/d/for-dataset/for-original/for-original/training/real'"
     ]
    }
   ],
   "source": [
    "folder_path_train_real = \"/mnt/d/for-dataset/for-original/for-original/training/real\"\n",
    "\n",
    "results_train = {}  # Store predictions\n",
    "\n",
    "for filename in os.listdir(folder_path_train_real):\n",
    "    if filename.endswith(\".wav\"):  # Only process audio files\n",
    "        file_path = os.path.join(folder_path_train_real, filename)\n",
    "        prediction = classify_audio(file_path)\n",
    "        results_train[filename] = prediction\n",
    "df_train = pd.DataFrame.from_dict(results_train, orient='index')  # rows = filenames\n",
    "df_train.index.name = 'filename'  # optional\n",
    "df_train.reset_index(inplace=True)  # make filename a column instead of index\n",
    "        \n",
    "df_gender_train=df_train.copy()\n",
    "\n",
    "df_prediction = df_gender_train[['female','male']].idxmax(axis=1)\n",
    "df_result_train = pd.concat([df_gender_train,df_prediction], axis=1)\n",
    "print(df_result_train)\n",
    "df_result_train=df_result_train.drop(df_result_train.columns[[1,2]],axis=1)\n",
    "df_result_train.rename(columns={0: 'predictions'}, inplace=True)\n",
    "df_result_train\n",
    "\n",
    "\n",
    "\n",
    "folder_path_test_real  = \"/mnt/d/for-dataset/for-original/for-original/testing/real\"\n",
    "results_test={}\n",
    "\n",
    "for filename in os.listdir(folder_path_test_real):\n",
    "    if filename.endswith(\".wav\"):  # Only process audio files\n",
    "        file_path = os.path.join(folder_path_test_real, filename)\n",
    "        prediction = classify_audio(file_path)\n",
    "        results_test[filename] = prediction\n",
    "df_test = pd.DataFrame.from_dict(results_test, orient='index')  # rows = filenames\n",
    "df_test.index.name = 'filename'  # optional\n",
    "df_test.reset_index(inplace=True)  # make filename a column instead of index\n",
    "\n",
    "df_gender_test=df_test.copy()\n",
    "\n",
    "df_prediction = df_gender_test[['female','male']].idxmax(axis=1)\n",
    "df_result_test = pd.concat([df_gender_test,df_prediction], axis=1)\n",
    "print(df_result_test)\n",
    "df_result_test=df_result_test.drop(df_result_test.columns[[1,2]],axis=1)\n",
    "df_result_test.rename(columns={0: 'predictions'}, inplace=True)\n",
    "df_result_test\n",
    "\n",
    "\n",
    "folder_path_validation_real  = \"/mnt/d/for-dataset/for-original/for-original/validation/real\"\n",
    "results_validation={}\n",
    "\n",
    "for filename in os.listdir(folder_path_validation_real):\n",
    "    if filename.endswith(\".wav\"):  # Only process audio files\n",
    "        file_path = os.path.join(folder_path_validation_real, filename)\n",
    "        prediction = classify_audio(file_path)\n",
    "        results_validation[filename] = prediction\n",
    "df_validation = pd.DataFrame.from_dict(results_validation, orient='index')  # rows = filenames\n",
    "df_validation.index.name = 'filename'  # optional\n",
    "df_validation.reset_index(inplace=True)  # make filename a column instead of index\n",
    "\n",
    "df_gender_validation=df_validation.copy()\n",
    "\n",
    "df_prediction = df_gender_validation[['female','male']].idxmax(axis=1)\n",
    "df_result_validation = pd.concat([df_gender_validation,df_prediction], axis=1)\n",
    "print(df_result_validation)\n",
    "df_result_validation=df_result_validation.drop(df_result_validation.columns[[1,2]],axis=1)\n",
    "df_result_validation.rename(columns={0: 'predictions'}, inplace=True)\n",
    "df_result_validation\n",
    "\n",
    "\n",
    "\n",
    "# Add dataset labels\n",
    "df_result_test['dataset'] = 'Test'\n",
    "df_result_train['dataset'] = 'Train'\n",
    "df_result_validation['dataset'] = 'Validation'   \n",
    "\n",
    "# Combine all three DataFrames\n",
    "combined = pd.concat([\n",
    "    df_result_test,\n",
    "    df_result_train,\n",
    "    df_result_validation\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "# Plot gender distribution grouped by dataset\n",
    "ax = sns.countplot(\n",
    "    x='predictions',\n",
    "    hue='dataset',\n",
    "    data=combined\n",
    ")\n",
    "\n",
    "# Add numbers on top of each bar\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(\n",
    "        p.get_x() + p.get_width()/2,\n",
    "        height + 0.05,\n",
    "        str(int(height)),\n",
    "        ha='center'\n",
    "    )\n",
    "\n",
    "plt.title('Gender Distribution: Train vs Test vs Validation for Real Samples')\n",
    "plt.xlabel('Predicted Gender')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_train_fake = \"/kaggle/input/the-fake-or-real-dataset/for-2sec/for-2seconds/training/fake\"\n",
    "\n",
    "results_train = {}  # Store predictions\n",
    "\n",
    "for filename in os.listdir(folder_path_train_fake):\n",
    "    if filename.endswith(\".wav\"):  # Only process audio files\n",
    "        file_path = os.path.join(folder_path_train_fake, filename)\n",
    "        prediction = classify_audio(file_path)\n",
    "        results_train[filename] = prediction\n",
    "df_train = pd.DataFrame.from_dict(results_train, orient='index')  # rows = filenames\n",
    "df_train.index.name = 'filename'  # optional\n",
    "df_train.reset_index(inplace=True)  # make filename a column instead of index\n",
    "        \n",
    "df_gender_train=df_train.copy()\n",
    "\n",
    "df_prediction = df_gender_train[['female','male']].idxmax(axis=1)\n",
    "df_result_train = pd.concat([df_gender_train,df_prediction], axis=1)\n",
    "print(df_result_train)\n",
    "df_result_train=df_result_train.drop(df_result_train.columns[[1,2]],axis=1)\n",
    "df_result_train.rename(columns={0: 'predictions'}, inplace=True)\n",
    "df_result_train\n",
    "\n",
    "\n",
    "\n",
    "folder_path_test_fake = \"/kaggle/input/the-fake-or-real-dataset/for-2sec/for-2seconds/testing/fake\"\n",
    "results_test={}\n",
    "\n",
    "for filename in os.listdir(folder_path_test_fake):\n",
    "    if filename.endswith(\".wav\"):  # Only process audio files\n",
    "        file_path = os.path.join(folder_path_test_fake, filename)\n",
    "        prediction = classify_audio(file_path)\n",
    "        results_test[filename] = prediction\n",
    "df_test = pd.DataFrame.from_dict(results_test, orient='index')  # rows = filenames\n",
    "df_test.index.name = 'filename'  # optional\n",
    "df_test.reset_index(inplace=True)  # make filename a column instead of index\n",
    "\n",
    "df_gender_test=df_test.copy()\n",
    "\n",
    "df_prediction = df_gender_test[['female','male']].idxmax(axis=1)\n",
    "df_result_test = pd.concat([df_gender_test,df_prediction], axis=1)\n",
    "print(df_result_test)\n",
    "df_result_test=df_result_test.drop(df_result_test.columns[[1,2]],axis=1)\n",
    "df_result_test.rename(columns={0: 'predictions'}, inplace=True)\n",
    "df_result_test\n",
    "\n",
    "\n",
    "folder_path_validation_fake  = \"/kaggle/input/the-fake-or-real-dataset/for-2sec/for-2seconds/validation/fake\"\n",
    "results_validation={}\n",
    "\n",
    "for filename in os.listdir(folder_path_validation_fake):\n",
    "    if filename.endswith(\".wav\"):  # Only process audio files\n",
    "        file_path = os.path.join(folder_path_validation_fake, filename)\n",
    "        prediction = classify_audio(file_path)\n",
    "        results_validation[filename] = prediction\n",
    "df_validation = pd.DataFrame.from_dict(results_validation, orient='index')  # rows = filenames\n",
    "df_validation.index.name = 'filename'  # optional\n",
    "df_validation.reset_index(inplace=True)  # make filename a column instead of index\n",
    "\n",
    "df_gender_validation=df_validation.copy()\n",
    "\n",
    "df_prediction = df_gender_validation[['female','male']].idxmax(axis=1)\n",
    "df_result_validation = pd.concat([df_gender_validation,df_prediction], axis=1)\n",
    "print(df_result_validation)\n",
    "df_result_validation=df_result_validation.drop(df_result_validation.columns[[1,2]],axis=1)\n",
    "df_result_validation.rename(columns={0: 'predictions'}, inplace=True)\n",
    "df_result_validation\n",
    "\n",
    "\n",
    "\n",
    "# Add dataset labels\n",
    "df_result_test['dataset'] = 'Test'\n",
    "df_result_train['dataset'] = 'Train'\n",
    "df_result_validation['dataset'] = 'Validation'   \n",
    "\n",
    "# Combine all three DataFrames\n",
    "combined = pd.concat([\n",
    "    df_result_test,\n",
    "    df_result_train,\n",
    "    df_result_validation\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "# Plot gender distribution grouped by dataset\n",
    "ax = sns.countplot(\n",
    "    x='predictions',\n",
    "    hue='dataset',\n",
    "    data=combined\n",
    ")\n",
    "\n",
    "# Add numbers on top of each bar\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(\n",
    "        p.get_x() + p.get_width()/2,\n",
    "        height + 0.05,\n",
    "        str(int(height)),\n",
    "        ha='center'\n",
    "    )\n",
    "\n",
    "plt.title('Gender Distribution: Train vs Test vs Validation for Fake Samples')\n",
    "plt.xlabel('Predicted Gender')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4555568,
     "sourceId": 8130934,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
