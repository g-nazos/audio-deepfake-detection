{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04cf0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "from utils.utils import save_experiment, train_and_evaluate_logistic_regression, train_and_evaluate_linear_svm, train_and_evaluate_non_linear_svm, train_and_evaluate_decision_tree, train_and_evaluate_random_forest, train_and_evaluate_xgboost\n",
    "from configs.config import DATASET_PATH, FEATURES_DIR, ITW_DATASET_PATH, MODELS_PATH\n",
    "\n",
    "from utils.utils import grid_search_joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d672f63",
   "metadata": {},
   "source": [
    "### Parquet paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0d3c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = os.path.join(FEATURES_DIR, \"training_features_mean_20_128_256_128.parquet\")\n",
    "val_data_path = os.path.join(FEATURES_DIR, \"testing_features_mean_20_128_256_128.parquet\")\n",
    "test_data_path = os.path.join(ITW_DATASET_PATH, 'normalized_features', \"itw_features_mean_20_128_256_128_trimmed_loudness_normalized.parquet\")\n",
    "\n",
    "#no mel features\n",
    "train_data_path_no_mel = os.path.join(FEATURES_DIR, \"training_features_mean_20_128_256_128_no_mel.parquet\")\n",
    "test_data_path_no_mel = os.path.join(FEATURES_DIR, \"testing_features_mean_20_128_256_128_no_mel.parquet\")\n",
    "itw_data_path_no_mel = os.path.join(ITW_DATASET_PATH, 'normalized_features', \"itw_features_mean_20_128_256_128_no_mel_trimmed_loudness_normalized.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18599dba",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a98da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        class_weight={0:1, 1:5},\n",
    "        random_state=42,\n",
    "        n_jobs=1,\n",
    "    )),\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"clf__solver\": [\"saga\"],\n",
    "    \"clf__penalty\": [\"l1\",\"l2\"],\n",
    "    \"clf__C\": np.logspace(-3, 2, 10),\n",
    "    \"clf__max_iter\": [1000],\n",
    "}\n",
    "\n",
    "(\n",
    "    final_model, \n",
    "    test_metrics, \n",
    "    val_metrics, \n",
    "    best_params, \n",
    "    val_results, \n",
    "    metadata, \n",
    "    feature_names  \n",
    ") = grid_search_joblib(\n",
    "    model,\n",
    "    param_grid,\n",
    "    train_data_path,\n",
    "    val_data_path,\n",
    "    test_data_path,\n",
    "    n_jobs=20\n",
    ")\n",
    "\n",
    "print(f'Best parameters:{best_params}')\n",
    "print(f'Resluts on validation data:{val_metrics}')\n",
    "print(f'Resluts on test data:{test_metrics}')\n",
    "\n",
    "save_experiment(\n",
    "    model=final_model,\n",
    "    metrics=test_metrics,\n",
    "    experiment_dir=os.path.join(sys.path[0], \"notebooks\", \"experiments\", \"final\", \"logistic_reg\",),\n",
    "    model_params=best_params,\n",
    "    feature_names=feature_names,\n",
    "    metadata_extra=metadata,\n",
    "    val_results=val_results,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ae807",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec23205",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm\",  LinearSVC(max_iter=20000, class_weight={0:1, 1:5}, random_state=42)),\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"svm__C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "}\n",
    "\n",
    "(\n",
    "    final_model, \n",
    "    test_metrics, \n",
    "    val_metrics, \n",
    "    best_params, \n",
    "    val_results, \n",
    "    metadata, \n",
    "    feature_names \n",
    ") = grid_search_joblib(\n",
    "    model,\n",
    "    param_grid,\n",
    "    train_data_path,\n",
    "    val_data_path,\n",
    "    test_data_path,\n",
    "    n_jobs=20\n",
    ")\n",
    "\n",
    "print(f'Best parameters:{best_params}')\n",
    "print(f'Resluts on validation data:{val_metrics}')\n",
    "print(f'Resluts on test data:{test_metrics}')\n",
    "\n",
    "save_experiment(\n",
    "    model=final_model,\n",
    "    metrics=test_metrics,\n",
    "    experiment_dir=os.path.join(sys.path[0], \"notebooks\", \"experiments\", \"final\", \"linear_svm\"),\n",
    "    model_params=best_params,\n",
    "    feature_names=feature_names,\n",
    "    metadata_extra=metadata,\n",
    "    val_results=val_results,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84642d75",
   "metadata": {},
   "source": [
    "### RBF SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13d1926",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm\", SVC(\n",
    "        class_weight={0: 1, 1: 5},\n",
    "        cache_size=2000,\n",
    "        random_state=42\n",
    "    )),\n",
    "])\n",
    "\n",
    "param_grid_rbf = {\n",
    "    \"svm__kernel\": [\"rbf\"],\n",
    "    \"svm__C\": [0.1, 1, 10, 100],\n",
    "    \"svm__gamma\": [\"scale\", 0.01, 0.1],\n",
    "}\n",
    "\n",
    "(\n",
    "    final_model_rbf, \n",
    "    test_metrics_rbf, \n",
    "    val_metrics_rbf, \n",
    "    best_params_rbf, \n",
    "    val_results_rbf, \n",
    "    metadata_rbf, \n",
    "    feature_names_rbf \n",
    ") = grid_search_joblib(\n",
    "    model,\n",
    "    param_grid_rbf,\n",
    "    train_data_path,\n",
    "    val_data_path,\n",
    "    test_data_path,\n",
    "    n_jobs=20\n",
    ")\n",
    "print(f'Best parameters:{best_params_rbf}')\n",
    "print(f'Resluts on validation data:{val_metrics_rbf}')\n",
    "print(f'Resluts on test data:{test_metrics_rbf}')\n",
    "\n",
    "save_experiment(\n",
    "    model=final_model_rbf,\n",
    "    metrics=test_metrics_rbf,\n",
    "    experiment_dir=os.path.join(sys.path[0], \"notebooks\", \"experiments\", \"final\", \"rbf_svm\"),\n",
    "    model_params=best_params_rbf,\n",
    "    feature_names=feature_names_rbf,\n",
    "    metadata_extra=metadata_rbf,\n",
    "    val_results=val_results_rbf,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a62051",
   "metadata": {},
   "source": [
    "### Poly SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6a605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm\", SVC(\n",
    "        class_weight={0: 1, 1: 5},\n",
    "        cache_size=2000,\n",
    "        random_state=42\n",
    "    )),\n",
    "])\n",
    "\n",
    "param_grid_poly = {\n",
    "    \"svm__kernel\": [\"poly\"],\n",
    "    \"svm__degree\": [2, 3],\n",
    "    \"svm__C\": [0.1, 1, 10],\n",
    "    \"svm__gamma\": [\"scale\", 0.01],\n",
    "    \"svm__coef0\": [0.0, 1.0],\n",
    "}\n",
    "\n",
    "(\n",
    "    final_model_poly, \n",
    "    test_metrics_poly, \n",
    "    val_metrics_poly, \n",
    "    best_params_poly, \n",
    "    val_results_poly, \n",
    "    metadata_poly, \n",
    "    feature_names_poly \n",
    ") = grid_search_joblib(\n",
    "    model,\n",
    "    param_grid_poly,\n",
    "    train_data_path,\n",
    "    val_data_path,\n",
    "    test_data_path,\n",
    "    n_jobs=20\n",
    ")\n",
    "print(f'Best parameters:{best_params_poly}')\n",
    "print(f'Resluts on validation data:{val_metrics_poly}')\n",
    "print(f'Resluts on test data:{test_metrics_poly}')\n",
    "\n",
    "save_experiment(\n",
    "    model=final_model_poly,\n",
    "    metrics=test_metrics_poly,\n",
    "    experiment_dir=os.path.join(sys.path[0], \"notebooks\", \"experiments\",  \"final\", \"poly_svm\"),\n",
    "    model_params=best_params_poly,\n",
    "    feature_names=feature_names_poly,\n",
    "    metadata_extra=metadata_poly,\n",
    "    val_results=val_results_poly,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcb025d",
   "metadata": {},
   "source": [
    "### Sigmoid Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb036435",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm\", SVC(\n",
    "        class_weight={0: 1, 1: 5},\n",
    "        cache_size=2000,\n",
    "        random_state=42\n",
    "    )),\n",
    "])\n",
    "\n",
    "param_grid_sigmoid = {\n",
    "    \"svm__kernel\": [\"sigmoid\"],\n",
    "    \"svm__C\": [0.01, 0.1, 1],\n",
    "    \"svm__gamma\": [\"scale\", 0.01],\n",
    "    \"svm__coef0\": [-1.0, 0.0, 1.0],\n",
    "}\n",
    "\n",
    "(\n",
    "    final_model_sigmoid, \n",
    "    test_metrics_sigmoid, \n",
    "    val_metrics_sigmoid, \n",
    "    best_params_sigmoid, \n",
    "    val_results_sigmoid, \n",
    "    metadata_sigmoid, \n",
    "    feature_names_sigmoid \n",
    ") = grid_search_joblib(\n",
    "    model,\n",
    "    param_grid_sigmoid,\n",
    "    train_data_path,\n",
    "    val_data_path,\n",
    "    test_data_path,\n",
    "    n_jobs=20\n",
    ")\n",
    "\n",
    "print(f'Best parameters:{best_params_sigmoid}')\n",
    "print(f'Resluts on validation data:{val_metrics_sigmoid}')\n",
    "print(f'Resluts on test data:{test_metrics_sigmoid}')\n",
    "\n",
    "save_experiment(\n",
    "    model=final_model_sigmoid,\n",
    "    metrics=test_metrics_sigmoid,\n",
    "    experiment_dir=os.path.join(sys.path[0], \"notebooks\", \"experiments\", \"final\", \"sigmoid_svm\"),\n",
    "    model_params=best_params_sigmoid,\n",
    "    feature_names=feature_names_sigmoid,\n",
    "    metadata_extra=metadata_sigmoid,\n",
    "    val_results=val_results_sigmoid,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d727bcbd",
   "metadata": {},
   "source": [
    "### Decission Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30dd261",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": [x for x in range(5, 20)],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"class_weight\": [{0: 1, 1: 5}, None],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"ccp_alpha\": [0.0, 1e-4, 1e-3, 1e-2],\n",
    "}\n",
    "\n",
    "\n",
    "(\n",
    "    final_model,\n",
    "    test_metrics,\n",
    "    val_metrics,\n",
    "    best_params,\n",
    "    val_results,\n",
    "    metadata,\n",
    "    feature_names\n",
    ") = grid_search_joblib(\n",
    "    model=model,\n",
    "    param_grid=params,\n",
    "    train_path=train_data_path,\n",
    "    val_path=val_data_path,\n",
    "    test_path=test_data_path,\n",
    "    scoring=\"f1_macro\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "save_experiment(\n",
    "    model=final_model,\n",
    "    metrics=test_metrics,\n",
    "    experiment_dir=os.path.join(sys.path[0], \"notebooks\", \"experiments\", \"final\", \"Dtree\",),\n",
    "    model_params=params,\n",
    "    feature_names=feature_names,\n",
    "    metadata_extra=metadata,\n",
    "    val_results=val_results,\n",
    ")\n",
    "print(metadata)\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872ba882",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c76dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "base_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('rf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "params = {\n",
    "    \"rf__n_estimators\": [300, 500, 700, 800],\n",
    "    \"rf__max_depth\": [2, 3, 4, 5],\n",
    "    \"rf__max_features\": [\"sqrt\", \"log2\"],\n",
    "    \"rf__min_samples_split\": [5, 10, 20],\n",
    "    \"rf__min_samples_leaf\": [2, 5, 10],\n",
    "    \"rf__max_samples\": [0.5, 0.6, 0.7],\n",
    "    \"rf__class_weight\": [{0: 1, 1: 5}, None],\n",
    "}\n",
    "\n",
    "(\n",
    "    final_model,\n",
    "    test_metrics,\n",
    "    val_metrics,\n",
    "    best_params,\n",
    "    val_results,\n",
    "    metadata,\n",
    "    feature_names\n",
    ") = grid_search_joblib(\n",
    "    model=base_pipe,\n",
    "    param_grid=params,\n",
    "    train_path=train_data_path,\n",
    "    val_path=val_data_path,\n",
    "    test_path=test_data_path,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "    \n",
    "save_experiment(\n",
    "    model=final_model,\n",
    "    metrics=test_metrics,\n",
    "    experiment_dir=os.path.join(sys.path[0], \"notebooks\", \"experiments\",  \"final\", \"RF\",),\n",
    "    model_params=best_params,\n",
    "    feature_names=feature_names,\n",
    "    metadata_extra=metadata,\n",
    "    val_results=val_results,\n",
    ")\n",
    "\n",
    "print(\"Metadata:\", metadata)\n",
    "print(\"Metrics:\", test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcbc977",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3c0406",
   "metadata": {},
   "source": [
    "#### XGBoost Mel features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad24f80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('xgb', XGBClassifier(eval_metric='aucpr', random_state=42))\n",
    "])\n",
    "\n",
    "params = {\n",
    "    # CORE ANTI-OVERFITTING: Very shallow trees\n",
    "    \"xgb__max_depth\": [3, 4, 2],  # Much shallower than DTree's 10\n",
    "    \n",
    "    # LEARNING: Slow and steady\n",
    "    \"xgb__learning_rate\": [0.03, 0.05],\n",
    "    \"xgb__n_estimators\": [700, 800],  # More trees, lower LR\n",
    "    \n",
    "    # REGULARIZATION: Aggressive to prevent dataset artifacts\n",
    "    \"xgb__min_child_weight\": [10, 20],  # Higher than default\n",
    "    \"xgb__gamma\": [0.3],  # Minimum loss reduction\n",
    "    \"xgb__reg_lambda\": [2, 1],  # L2 regularization\n",
    "    \"xgb__reg_alpha\": [0.5],  # L1 regularization\n",
    "    \n",
    "    # SAMPLING: Reduce correlation and overfitting\n",
    "    \"xgb__subsample\": [0.6, 0.7, 0.5],  # Row sampling\n",
    "    \"xgb__colsample_bytree\": [0.6, 0.7],  # Feature sampling\n",
    "    \n",
    "    # CLASS IMBALANCE: More moderate than 1:5\n",
    "    \"xgb__scale_pos_weight\": [2],  # or calculate actual ratio\n",
    "    \n",
    "    # Fixed params\n",
    "    \"xgb__objective\": [\"binary:logistic\"],\n",
    "    #\"eval_metric\": [\"logloss\"],\n",
    "    #\"tree_method\": [\"hist\"],  # Fast\n",
    "}\n",
    "\n",
    "(\n",
    "    final_model,\n",
    "    test_metrics,\n",
    "    val_metrics,\n",
    "    best_params,\n",
    "    val_results,\n",
    "    metadata,\n",
    "    feature_names\n",
    ") = grid_search_joblib(\n",
    "    model=base_pipe,\n",
    "    param_grid=params,\n",
    "    train_path=train_data_path,\n",
    "    val_path=val_data_path,\n",
    "    test_path=test_data_path,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "save_experiment(\n",
    "    model=final_model,\n",
    "    metrics=test_metrics,\n",
    "    experiment_dir=os.path.join(sys.path[0], \"notebooks\", \"experiments\", \"final\", \"XGB\"),\n",
    "    model_params=best_params,\n",
    "    feature_names=feature_names,\n",
    "    metadata_extra=metadata,\n",
    "    val_results=val_results,\n",
    ")\n",
    "\n",
    "print(\"Metadata:\", metadata)\n",
    "print(\"Metrics:\", test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f48532",
   "metadata": {},
   "source": [
    "#### XGBoost without mel features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b567f037",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('xgb', XGBClassifier(eval_metric='aucpr', random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    # CORE ANTI-OVERFITTING: Very shallow trees\n",
    "    \"xgb__max_depth\": [3, 4, 5],  # ← xgb__ prefix\n",
    "    \n",
    "    # LEARNING: Slow and steady\n",
    "    \"xgb__learning_rate\": [0.01, 0.03, 0.05],\n",
    "    \"xgb__n_estimators\": [300, 500, 700],\n",
    "    \n",
    "    # REGULARIZATION: Aggressive to prevent dataset artifacts\n",
    "    \"xgb__min_child_weight\": [5, 10, 20],\n",
    "    \"xgb__gamma\": [0.1, 0.3, 0.5],\n",
    "    \"xgb__reg_lambda\": [2, 5, 10],  # Note: 'lambda' → 'reg_lambda'\n",
    "    \"xgb__reg_alpha\": [0, 0.5, 1],  # Note: 'alpha' → 'reg_alpha'\n",
    "    \n",
    "    # SAMPLING: Reduce correlation and overfitting\n",
    "    \"xgb__subsample\": [0.6, 0.7, 0.8],\n",
    "    \"xgb__colsample_bytree\": [0.6, 0.7, 0.8],\n",
    "    \n",
    "    # CLASS IMBALANCE: More moderate than 1:5\n",
    "    \"xgb__scale_pos_weight\": [1, 2, 3],\n",
    "    \n",
    "    # Fixed params - NO prefix needed (or include with prefix)\n",
    "    # \"xgb__tree_method\": [\"hist\"],\n",
    "    # \"xgb__random_state\": [42],  # Already set in XGBClassifier above\n",
    "}\n",
    "\n",
    "\n",
    "(\n",
    "    final_model,\n",
    "    test_metrics,\n",
    "    val_metrics,\n",
    "    best_params,\n",
    "    val_results,\n",
    "    metadata,\n",
    "    feature_names\n",
    ") = grid_search_joblib(\n",
    "    model=base_pipe,\n",
    "    param_grid=param_grid,\n",
    "    train_path=train_data_path_no_mel,\n",
    "    val_path=test_data_path_no_mel,\n",
    "    test_path=itw_data_path_no_mel,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "save_experiment(\n",
    "    model=final_model,\n",
    "    metrics=test_metrics,\n",
    "    experiment_dir=os.path.join(sys.path[0], \"notebooks\", \"experiments\", \"final\", \"XGB_NO_MEL\"),\n",
    "    model_params=best_params,\n",
    "    feature_names=feature_names,\n",
    "    metadata_extra=metadata,\n",
    "    val_results=val_results,\n",
    ")\n",
    "\n",
    "print(\"Metadata:\", metadata)\n",
    "print(\"Metrics:\", test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retrain_final_md",
   "metadata": {},
   "source": [
    "### Retrain Final Models on FoR (Training → Test)\n",
    "\n",
    "For each classifier, we load the best hyperparameters from the final experiment's `model_params.json` and retrain the model using the FoR **training** set. The model is then evaluated on the FoR **test** set so that confusion matrices and ROC curves remain available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retrain_final_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "train_data_path = os.path.join(FEATURES_DIR, \"training_features_mean_20_128_256_128.parquet\")\n",
    "val_data_path = os.path.join(FEATURES_DIR, \"validation_features_mean_20_128_256_128.parquet\")\n",
    "test_data_path = os.path.join(FEATURES_DIR, \"testing_features_mean_20_128_256_128.parquet\")\n",
    "\n",
    "train_data_path_no_mel = os.path.join(FEATURES_DIR, \"training_features_mean_20_128_256_128_no_mel.parquet\")\n",
    "val_data_path_no_mel = os.path.join(FEATURES_DIR, \"validation_features_mean_20_128_256_128_no_mel.parquet\")\n",
    "test_data_path_no_mel = os.path.join(FEATURES_DIR, \"testing_features_mean_20_128_256_128_no_mel.parquet\")\n",
    "\n",
    "FINAL_MODELS_PATH = os.path.join(\"experiments\", \"final\")\n",
    "FOR_MODELS_PATH = os.path.join(\"experiments\", \"FoR\")\n",
    "\n",
    "# ── Helper: strip pipeline prefixes from param keys ──\n",
    "def strip_prefix(params_dict):\n",
    "    stripped = {}\n",
    "    for k, v in params_dict.items():\n",
    "        for pfx in (\"clf__\", \"svm__\", \"rf__\", \"xgb__\"):\n",
    "            if k.startswith(pfx):\n",
    "                k = k[len(pfx):]\n",
    "                break\n",
    "        stripped[k] = v\n",
    "    return stripped\n",
    "\n",
    "def fix_class_weight(params):\n",
    "    \"\"\"Convert class_weight dict keys from str to int (JSON artefact).\"\"\"\n",
    "    cw = params.get(\"class_weight\")\n",
    "    if isinstance(cw, dict):\n",
    "        params[\"class_weight\"] = {int(k): v for k, v in cw.items()}\n",
    "    return params\n",
    "\n",
    "# ── Registry: (model_name, subdir, save_subdir, model_type, uses_no_mel) ──\n",
    "MODELS = [\n",
    "    (\"Logistic Regression\",  \"logistic_reg/exp_20260207_192945\",   \"logistic_reg\",  \"lr\",         False),\n",
    "    (\"Linear SVM\",           \"linear_svm/exp_20260207_193304\",     \"linear_svm\",    \"linear_svm\", False),\n",
    "    (\"RBF SVM\",              \"rbf_svm/exp_20260207_200752\",        \"rbf_svm\",       \"nl_svm\",     False),\n",
    "    (\"Poly SVM\",             \"poly_svm/exp_20260207_201742\",       \"poly_svm\",      \"nl_svm\",     False),\n",
    "    (\"Sigmoid SVM\",          \"sigmoid_svm/exp_20260207_204305\",    \"sigmoid_svm\",   \"nl_svm\",     False),\n",
    "    (\"Decision Tree\",        \"Dtree/exp_20260207_210558\",          \"Dtree\",         \"dtree\",      False),\n",
    "    (\"Random Forest\",        \"RF/exp_20260208_163746\",             \"RF\",            \"rf\",         False),\n",
    "    (\"XGBoost\",              \"XGB/exp_20260208_160623\",            \"XGB\",           \"xgb\",        False),\n",
    "    (\"XGBoost (no mel)\",     \"XGB_NO_MEL/exp_20260208_034900\",    \"XGB_NO_MEL\",    \"xgb\",        True),\n",
    "]\n",
    "\n",
    "# ── Train each model with best params and save ──\n",
    "trained_models = {}\n",
    "\n",
    "for model_name, subdir, save_subdir, model_type, no_mel in MODELS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    exp_dir = os.path.join(FINAL_MODELS_PATH, subdir)\n",
    "\n",
    "    # Select data paths\n",
    "    if no_mel:\n",
    "        t_train, t_test, t_val = train_data_path_no_mel, test_data_path_no_mel, val_data_path_no_mel\n",
    "    else:\n",
    "        t_train, t_test, t_val = train_data_path, test_data_path, val_data_path\n",
    "\n",
    "    # Load model_params.json\n",
    "    with open(os.path.join(exp_dir, \"model_params.json\")) as f:\n",
    "        raw_params = json.load(f)\n",
    "\n",
    "    # Decision Tree: model_params.json has the grid, not best params\n",
    "    if model_type == \"dtree\":\n",
    "        with open(os.path.join(exp_dir, \"val_results.json\")) as f:\n",
    "            val_results = json.load(f)\n",
    "        best_entry = max(val_results, key=lambda x: x[\"selection_score\"])\n",
    "        params = strip_prefix(best_entry[\"params\"])\n",
    "    else:\n",
    "        params = strip_prefix(raw_params)\n",
    "\n",
    "    params = fix_class_weight(params)\n",
    "    print(f\"Params: {params}\")\n",
    "\n",
    "    # ── Train & evaluate ──\n",
    "    if model_type == \"lr\":\n",
    "        params.setdefault(\"class_weight\", {0:1, 1:5})\n",
    "        params.setdefault(\"random_state\", 42)\n",
    "        pipeline, metrics, used_params, features, meta = train_and_evaluate_logistic_regression(\n",
    "            t_train, t_test, lr_params=params\n",
    "        )\n",
    "\n",
    "    elif model_type == \"linear_svm\":\n",
    "        params.setdefault(\"class_weight\", {0:1, 1:5})\n",
    "        params.setdefault(\"max_iter\", 20000)\n",
    "        params.setdefault(\"random_state\", 42)\n",
    "        pipeline, metrics, used_params, features, meta = train_and_evaluate_linear_svm(\n",
    "            t_train, t_test, svc_params=params\n",
    "        )\n",
    "\n",
    "    elif model_type == \"nl_svm\":\n",
    "        params.setdefault(\"class_weight\", {0:1, 1:5})\n",
    "        params.setdefault(\"max_iter\", 20000)\n",
    "        params.setdefault(\"random_state\", 42)\n",
    "        pipeline, metrics, used_params, features, meta = train_and_evaluate_non_linear_svm(\n",
    "            t_train, t_test, svc_params=params\n",
    "        )\n",
    "\n",
    "    elif model_type == \"dtree\":\n",
    "        criterion = params.pop(\"criterion\", \"gini\")\n",
    "        params.setdefault(\"random_state\", 42)\n",
    "        pipeline, metrics, used_params, features, meta = train_and_evaluate_decision_tree(\n",
    "            t_train, test_path=t_test, dt_params=params, criterion=criterion\n",
    "        )\n",
    "\n",
    "    elif model_type == \"rf\":\n",
    "        params.setdefault(\"random_state\", 42)\n",
    "        params.setdefault(\"n_jobs\", -1)\n",
    "        pipeline, metrics, used_params, features, meta, _oob = train_and_evaluate_random_forest(\n",
    "            t_train, val_path=t_val, test_path=t_test, rf_params=params\n",
    "        )\n",
    "\n",
    "    elif model_type == \"xgb\":\n",
    "        pipeline, metrics, used_params, features, meta = train_and_evaluate_xgboost(\n",
    "            t_train, val_path=t_val, test_path=t_test, xgb_params=params\n",
    "        )\n",
    "\n",
    "    trained_models[model_name] = {\"pipeline\": pipeline, \"metrics\": metrics}\n",
    "    print(f\"  Accuracy: {metrics['accuracy']:.4f}  |  F1: {metrics['f1']:.4f}  |  ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "\n",
    "    # ── Save experiment under experiments/for/<model_subdir> ──\n",
    "    save_experiment(\n",
    "        model=pipeline,\n",
    "        metrics=metrics,\n",
    "        experiment_dir=os.path.join(FOR_MODELS_PATH, save_subdir),\n",
    "        model_params=used_params,\n",
    "        feature_names=features,\n",
    "        metadata_extra=meta,\n",
    "    )\n",
    "\n",
    "# ── Summary table ──\n",
    "print(f\"\\n\\n{'='*90}\")\n",
    "print(\"  SUMMARY: FoR Test Set Results (retrained with best params)\")\n",
    "print(f\"{'='*90}\")\n",
    "print(f\"{'Model':<25} {'Accuracy':>10} {'Precision':>10} {'Recall':>10} {'F1':>10} {'ROC AUC':>10}\")\n",
    "print(\"-\" * 90)\n",
    "for name, data in trained_models.items():\n",
    "    m = data[\"metrics\"]\n",
    "    print(f\"{name:<25} {m['accuracy']:>10.4f} {m['precision']:>10.4f} {m['recall']:>10.4f} {m['f1']:>10.4f} {m['roc_auc']:>10.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492a8038",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = os.path.join(FEATURES_DIR, \"validation_features_mean_20_128_256_128.parquet\")\n",
    "\n",
    "for_val = pd.read_parquet(val)\n",
    "\n",
    "for_no_mel = for_val.loc[:, ~for_val.columns.str.startswith(\"mel_spectrogram\")]\n",
    "\n",
    "save_no_mel_data_path = os.path.join(FEATURES_DIR, \"validation_features_mean_20_128_256_128_no_mel.parquet\")\n",
    "for_no_mel .to_parquet(save_no_mel_data_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
